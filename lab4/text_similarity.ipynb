{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity Metrics\n",
    "\n",
    "Exercise notebook\n",
    "\n",
    "Course: Algorytmy Tekstowe at AGH University"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and vectorization\n",
    "\n",
    "1. Preprocessing: Convert the text documents to lowercase and remove all punctuation marks (using regular expressions, for example).\n",
    "2. Vocabulary creation: Create a vocabulary by taking all unique words from all text documents.\n",
    "3. Word frequency vectors: Create two vectors, each representing the frequency of each word in the vocabulary in each text document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sortedcontainers import SortedSet\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    marks = ['.', ';', ':']\n",
    "    for mark in marks:\n",
    "        text = text.translate({ord(mark):None})\n",
    "    # Your code here:\n",
    "    # Convert the text to lowercase.\n",
    "    # Remove all punctuation marks;\n",
    "    return text\n",
    "\n",
    "def text_to_vec(docs: list[str]) -> list[list[int]]:\n",
    "    # Your code here:\n",
    "    # Convert documents to numerical vectors.\n",
    "    # Preprocess documents with the preprocess() function.\n",
    "    # Represent documents as vectors of word frequencies, \n",
    "    # you will need to extract a vocabulary from all the documents.\n",
    "    alphabet = SortedSet()\n",
    "    sentences = []\n",
    "\n",
    "    for doc in docs:\n",
    "        sentences.append([])\n",
    "        sentences[-1] = preprocess(doc).split()\n",
    "        alphabet.update(sentences[-1])\n",
    "    \n",
    "    freq_vecs = []    \n",
    "    n = len(alphabet)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        freq_vecs.append([0] * n)\n",
    "        for word in sentence:\n",
    "            freq_vecs[-1][alphabet.index(word)] += 1\n",
    "\n",
    "    return freq_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "text_a = \"The quick brown fox jumped over the lazy dog.\"\n",
    "text_b = \"The lazy dog was jumped over by the quick brown fox.\"\n",
    "vec_a, vec_b = text_to_vec([text_a, text_b])\n",
    "\n",
    "\n",
    "assert(set(vec_a) == set([1, 1, 1, 2, 1, 1, 1, 1, 0, 0]))\n",
    "assert(set(vec_b) == set([1, 1, 1, 2, 1, 1, 1, 1, 1, 1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}= \\frac{\\sum\\limits_{i=1}^{n} A_i B_i}{\\sqrt{\\sum\\limits_{i=1}^{n} A_i^2} \\sqrt{\\sum\\limits_{i=1}^{n} B_i^2}}\n",
    "    \\qquad\\begin{aligned}\n",
    "    &\\text{where:} \\\\\n",
    "    &\\mathbf{A}\\text{ and }\\mathbf{B} \\text{ are the two vectors being compared}\\\\\n",
    "    &n \\text{ is the dimensionality of the vectors}\\\\\n",
    "    &\\theta \\text{ represents the angle between two vectors } \\mathbf{A} \\text{ and } \\mathbf{B} \\text{ in a high-dimensional space}\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The dot product of $\\mathbf{A}$ and $\\mathbf{B}$ is divided by the product of their Euclidean lengths to normalize the result to a range of [-1, 1]. A value of 1 indicates that the two vectors are identical, while a value of -1 indicates that they are completely dissimilar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(text_a: str, text_b: str) -> float:\n",
    "    # Your code here:\n",
    "    # Implement the cosine similarity\n",
    "    cosine_similarity = 0\n",
    "    vec_a, vec_b = text_to_vec([text_a, text_b])\n",
    "    vec_a, vec_b = np.array(vec_a), np.array(vec_b)\n",
    "    \n",
    "    cosine_similarity = vec_a.dot(vec_b)/(np.linalg.norm(vec_a)*np.linalg.norm(vec_b))\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "dist = cosine_similarity(text_a, text_b)\n",
    "assert(abs(dist - 0.91986) < 0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice coefficient / Sørensen-Dice Index\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\text{Dice}(A, B) = \\frac{2 |A \\cap B|}{|A| + |B|} \n",
    "    \\qquad\\begin{aligned}\n",
    "    &\\text{where:} \\\\\n",
    "    &A \\text{ and } B \\text{ represent the two sets being compared} \\\\\n",
    "    &|A| \\text{ and } |B| \\text{ represent the cardinality (number of elements) of the sets} \\\\\n",
    "    &\\text{and } |A \\cap B| \\text{ represents the size of the intersection of the two sets}\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dice(text_a: str, text_b: str) -> float:\n",
    "    # Your code here:\n",
    "    # Implement the Dice coefficient\n",
    "    dice = 0\n",
    "    set_A = set(preprocess(text_a).split())\n",
    "    set_B = set(preprocess(text_b).split())\n",
    "    dice = 2*len(set_A.intersection(set_B)) / (len(set_A) + len(set_B))\n",
    "    return dice\n",
    "\n",
    "dice(text_a, text_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "dist = dice(text_a, text_b)\n",
    "assert(abs(dist - 0.88888) < 0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean distance\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}\n",
    "    \\qquad\\begin{aligned}\n",
    "    &\\text{where:} \\\\\n",
    "    &d(x,y) \\text{ is the Euclidean distance} \\\\\n",
    "    &x_i, y_i \\text{ are the values of the i-th dimension of vectors } x \\text{ and } y \\\\\n",
    "    &n \\text{ is the number of dimensions in the vectors}\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(text_a: str, text_b: str) -> float:\n",
    "    # Your code here:\n",
    "    # Implement the Euclidean distance\n",
    "    dist = 0\n",
    "    vec_a, vec_b = text_to_vec([text_a, text_b])\n",
    "    vec_a, vec_b = np.array(vec_a), np.array(vec_b)\n",
    "\n",
    "    dist = np.linalg.norm(vec_a - vec_b)\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "\n",
    "dist = euclidean_distance(text_a, text_b)\n",
    "assert(abs(dist - 1.4142135) < 0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCS - Longest Common Subsequence\n",
    "\n",
    "Longest, common, continuous subsequence of two sequences, aka \"the longest substring\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miacierz najdłuższego wspólnego podciągu - tablica w której dynamicznie zapisujemy \n",
    "# dlóugość najdłuższego wspólnego podciągu między prefiksami słów \"s\" i \"t\"\n",
    "def lcs_matrix(s, t, key=lambda x:x):\n",
    "    m, n = len(s), len(t)\n",
    "    # inicjalizacja macierzy\n",
    "    d = [[0] * (n + 1) for i in range(m + 1)]\n",
    "    # wypłnianie wartościami\n",
    "    for j in range(1,n+1):\n",
    "        for i in range(1,m+1):\n",
    "            if key(s[i-1]) == key(t[j-1]):\n",
    "                d[i][j] = d[i-1][j-1] + 1\n",
    "            else:\n",
    "                d[i][j] = max(d[i][j-1], d[i-1][j])\n",
    "    return d\n",
    "\n",
    "#długość najdłurzszego wspólnego podciągu O(nm)\n",
    "def lcs_len(s, t, key=lambda x:x):\n",
    "    m, n = len(s), len(t)\n",
    "    d = lcs_matrix(s, t, key)\n",
    "    return d[m][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Sequence\n",
    "\n",
    "def lcs(seq_a: Sequence[Any], seq_b: Sequence[Any]) -> int:\n",
    "    # Your code here:\n",
    "    # Implement the longest common subsequence calculation.\n",
    "    # It should work on any sequences, not only on strings.\n",
    "    lcs = 0\n",
    "    lcs = lcs_len(seq_a, seq_b)\n",
    "    return lcs\n",
    "\n",
    "def word_lcs(text_a: str, text_b: str) -> int:\n",
    "    # You code here:\n",
    "    # Using the above function implement the LCS algorithm for texts.\n",
    "    # Make sure it works on whole words, not on characters.\n",
    "    seq_a = []\n",
    "    seq_b = []\n",
    "\n",
    "    seq_a = preprocess(text_a).split()\n",
    "    seq_b = preprocess(text_b).split()\n",
    "    return lcs(seq_a, seq_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "assert lcs(\"banana\", \"ananas\") == 5\n",
    "assert word_lcs(text_a, text_b) == 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein distance\n",
    "\n",
    "The minimal number of operations that needs to be performed in order to turn sequence A into sequence B.\n",
    "\n",
    "Available operations:\n",
    "\n",
    "* Replace element\n",
    "* Remove element\n",
    "* Add element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miacierz odlgłości edycyjnej - tablica w której dynamicznie zapisujemy \n",
    "# odległość edycyjną między prefiksami słów \"s\" i \"t\"\n",
    "def levenshtein_distance_matrix(s, t):\n",
    "    m, n = len(s), len(t)\n",
    "    # inicjalizacja macierzy\n",
    "    d = [[0] * (n + 1) for i in range(m + 1)]\n",
    "    for i in range(m + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        d[0][j] = j\n",
    "    # wypełnienie macierzy\n",
    "    for j in range(1, n + 1):\n",
    "        for i in range(1, m + 1):\n",
    "            if s[i - 1] == t[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                d[i][j] = min(d[i - 1][j], d[i][j - 1], d[i - 1][j - 1]) + 1\n",
    "    return d\n",
    "\n",
    "# odległość edycyjna między \"s\" i \"t\"\n",
    "def levenshtein_distance(s, t):\n",
    "    m, n = len(s), len(t)\n",
    "    return levenshtein_distance_matrix(s,t)[m][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def levenshtein(seq_a: Sequence[Any], seq_b: Sequence[Any]) -> int:\n",
    "    # Your code here:\n",
    "    # Implement the Levenshtein distance calculation.\n",
    "    # It should work on any sequences, not only on strings.\n",
    "\n",
    "    dist = levenshtein_distance(seq_a, seq_b)\n",
    "\n",
    "    return dist\n",
    "\n",
    "\n",
    "def word_levenshtein(text_a: str, text_b: str) -> int:\n",
    "    # You code here:\n",
    "    # Using the above function implement the LCS algorithm for texts.\n",
    "    # Make sure it works on whole words, not on characters.\n",
    "    seq_a = []\n",
    "    seq_b = []\n",
    "    \n",
    "    seq_a = preprocess(text_a).split()\n",
    "    seq_b = preprocess(text_b).split()\n",
    "    \n",
    "    return levenshtein(seq_a, seq_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "assert levenshtein(\"banana\", \"ananas\") == 2\n",
    "assert word_levenshtein(text_a, text_b) == 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
